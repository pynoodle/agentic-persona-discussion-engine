#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG Manager - LangChain ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ ì§€ì‹ ê´€ë¦¬
ê° í˜ë¥´ì†Œë‚˜ë³„ ë…ë¦½ì ì¸ ë²¡í„°ìŠ¤í† ì–´ì™€ retriever ìƒì„±
"""

import os
from pathlib import Path
from typing import List, Dict, Optional
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.document_loaders import TextLoader, DirectoryLoader

def safe_print(msg):
    """Windows ì¸ì½”ë”© ì˜¤ë¥˜ ë°©ì§€ìš© ì•ˆì „í•œ print"""
    try:
        print(msg)
    except UnicodeEncodeError:
        # ì´ëª¨ì§€ ì œê±° í›„ ì¶œë ¥
        import re
        clean_msg = re.sub(r'[^\x00-\x7F]+', '', msg)
        print(clean_msg)

class RAGManager:
    """í˜ë¥´ì†Œë‚˜ë³„ RAG ì‹œìŠ¤í…œ ê´€ë¦¬ì"""
    
    def __init__(self, use_openai_embeddings=True):
        """
        RAG ê´€ë¦¬ì ì´ˆê¸°í™”
        
        Args:
            use_openai_embeddings: Trueë©´ OpenAI (ìš”êµ¬ì‚¬í•­), Falseë©´ HuggingFace
        """
        self.data_dir = Path(__file__).parent / "data"
        # Use new vector stores with updated content
        self.vector_store_dir = Path(__file__).parent / "vector_stores_new"
        self.vector_store_dir.mkdir(exist_ok=True)
        
        # OpenAI API í‚¤ í™•ì¸
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # OpenAI Embeddings ì‚¬ìš© (ìš”êµ¬ì‚¬í•­)
        try:
            print("[*] OpenAI Embeddings initializing...")
        except:
            pass
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002"
        )
        
        # LLM (OpenAI GPT-4)
        self.llm = ChatOpenAI(
            model_name="gpt-4",
            temperature=0.7,
            max_tokens=500
        )
        
        # Text Splitter (ìš”êµ¬ì‚¬í•­: chunk_size=500, overlap=50)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        
        # í˜ë¥´ì†Œë‚˜ë³„ Vector Store & Retriever (LangChain 1.0 - qa_chains ì œê±°)
        self.vector_stores = {}
        self.retrievers = {}
        
        # í˜ë¥´ì†Œë‚˜ ì •ì˜ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
        self.personas = {
            # ê³ ê° í˜ë¥´ì†Œë‚˜ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
            'customer_foldable_enthusiast': 'í´ë”ë¸”ë§¤ë ¥íŒŒ (564ëª…, 63.2 ì¢‹ì•„ìš”)',
            'customer_ecosystem_dilemma': 'ìƒíƒœê³„ë”œë ˆë§ˆ (37ëª…, 31.0 ì¢‹ì•„ìš”)',
            'customer_foldable_critical': 'í´ë”ë¸”ë¹„íŒì (80ëª…, 7.74 ì¢‹ì•„ìš”)',
            'customer_upgrade_cycler': 'ì •ê¸°ì—…ê·¸ë ˆì´ë” (58ëª…, 6.88 ì¢‹ì•„ìš”)',
            'customer_value_seeker': 'ê°€ì„±ë¹„ì¶”êµ¬ì (8ëª…, 376.75 ì¢‹ì•„ìš”)',
            'customer_apple_ecosystem_loyal': 'Appleìƒíƒœê³„ì¶©ì„± (79ëª…, 12.56 ì¢‹ì•„ìš”)',
            'customer_design_fatigue': 'ë””ìì¸í”¼ë¡œ (48ëª…, 11.42 ì¢‹ì•„ìš”)',
            
            # ì„ì§ì› í˜ë¥´ì†Œë‚˜ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)
            'employee_marketer': 'ìµœì§€í›ˆ ë§ˆì¼€í„° (MXì‚¬ì—…ë¶€ ë§ˆì¼€íŒ… ì´ê´„ ì´ì‚¬)',
            'employee_developer': 'ë°•ì¤€í˜¸ ì—”ì§€ë‹ˆì–´ (í•˜ë“œì›¨ì–´ ë° ì„±ëŠ¥ ìµœì í™” ìµœê³  ì±…ì„ì)',
            'employee_designer': 'ì´í˜„ì„œ ë””ìì´ë„ˆ (ë””ìì¸ ì „ëµ ì´ê´„ / ë¦¬ë“œ ë””ìì´ë„ˆ)',
            
            # ì œí’ˆ ì •ë³´ (ì „ë°˜ì  í† ë¡  ì°¸ê³ ìš©)
            'product_info': 'ê°¤ëŸ­ì‹œ Z í´ë“œ 7 & Z í”Œë¦½ 7 ì œí’ˆ ì •ë³´'
        }
        
        try:
            print("[OK] RAG Manager initialized")
            print("   - Embeddings: OpenAI (text-embedding-ada-002)")
            print("   - Chunk Size: 500, Overlap: 50")
            print("   - Vector Store: ChromaDB")
        except:
            pass
    
    def load_persona_knowledge(self, persona_name: str) -> Optional[Chroma]:
        """
        í˜ë¥´ì†Œë‚˜ ì§€ì‹ ë¡œë“œ ë° ë²¡í„°í™”
        
        Args:
            persona_name: í˜ë¥´ì†Œë‚˜ ì´ë¦„ (ì˜ˆ: 'customer_iphone_to_galaxy')
        
        Returns:
            Chroma ë²¡í„° ìŠ¤í† ì–´ ê°ì²´
        """
        file_path = self.data_dir / f"{persona_name}.txt"
        
        if not file_path.exists():
            safe_print(f"[!] {file_path} file not found")
            return None
        
        safe_print(f"[*] Loading {persona_name}.txt...")
        
        # ë¬¸ì„œ ë¡œë“œ (TextLoader ì‚¬ìš©)
        loader = TextLoader(str(file_path), encoding='utf-8')
        documents = loader.load()
        
        # í…ìŠ¤íŠ¸ ë¶„í•  (ìš”êµ¬ì‚¬í•­: chunk_size=500, overlap=50)
        chunks = self.text_splitter.split_documents(documents)
        
        safe_print(f"    Split into {len(chunks)} chunks (500 chars/chunk, 50 overlap)")
        
        # Vector Store ìƒì„± (Chroma DB, OpenAI Embeddings)
        vector_store_path = str(self.vector_store_dir / persona_name)
        
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±
        if (self.vector_store_dir / persona_name).exists():
            safe_print(f"    Loading existing vector store...")
            vector_store = Chroma(
                persist_directory=vector_store_path,
                embedding_function=self.embeddings
            )
        else:
            safe_print(f"    Creating new vector store...")
            vector_store = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=vector_store_path
            )
            safe_print(f"    Vector store saved")
        
        # Vector Store ì €ì¥
        self.vector_stores[persona_name] = vector_store
        
        # Retriever ìƒì„± (ë³„ë„ ì €ì¥)
        retriever = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}  # Top 3 ê´€ë ¨ ë¬¸ì„œ
        )
        self.retrievers[persona_name] = retriever
        
        # QA Chain ìƒì„± (LangChain 1.0 LCEL ë°©ì‹)
        # RAG Chainì„ ì—¬ê¸°ì„œ ìƒì„±í•˜ì§€ ì•Šê³ , query_personaì—ì„œ ìƒì„±
        
        safe_print(f"[OK] {self.personas[persona_name]} ready")
        safe_print(f"    - Chunks: {len(chunks)}")
        safe_print(f"    - Retriever: similarity search (k=3)")
        safe_print(f"    - Vector store: {vector_store_path}")
        
        return vector_store
    
    def load_all_personas(self):
        """ëª¨ë“  í˜ë¥´ì†Œë‚˜ ì§€ì‹ ë¡œë“œ"""
        safe_print("\n" + "="*80)
        safe_print("[*] Loading all persona knowledge...")
        safe_print("="*80 + "\n")
        
        for persona_name in self.personas.keys():
            self.load_persona_knowledge(persona_name)
            safe_print("")  # ë¹ˆ ì¤„
        
        safe_print("="*80)
        safe_print(f"[OK] Total {len(self.vector_stores)} personas ready")
        safe_print(f"   - Vector Stores: {len(self.vector_stores)}")
        safe_print(f"   - Retrievers: {len(self.retrievers)}")
        safe_print("="*80 + "\n")
    
    def get_context(self, persona_type: str, query: str, k: int = 3) -> List[str]:
        """
        íŠ¹ì • í˜ë¥´ì†Œë‚˜ì˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ìš”êµ¬ì‚¬í•­ ë©”ì„œë“œëª…)
        
        Args:
            persona_type: í˜ë¥´ì†Œë‚˜ íƒ€ì… (ì˜ˆ: 'customer_iphone_to_galaxy')
            query: ê²€ìƒ‰ ì§ˆì˜
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        
        Returns:
            ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        if persona_type not in self.retrievers:
            safe_print(f"[!] Retriever not found for '{persona_type}'")
            return []
        
        # Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ (LangChain 1.0 - invoke ì‚¬ìš©)
        docs = self.retrievers[persona_type].invoke(query)
        
        # ìƒìœ„ kê°œë§Œ ë°˜í™˜
        return [doc.page_content for doc in docs[:k]]
    
    def get_relevant_context(self, persona_name: str, query: str, k: int = 3) -> List[str]:
        """
        íŠ¹ì • í˜ë¥´ì†Œë‚˜ì˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (í•˜ìœ„ í˜¸í™˜ì„±)
        
        Args:
            persona_name: í˜ë¥´ì†Œë‚˜ ì´ë¦„
            query: ê²€ìƒ‰ ì§ˆì˜
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        
        Returns:
            ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        # get_context í˜¸ì¶œ (ë™ì¼ ê¸°ëŠ¥)
        return self.get_context(persona_name, query, k)
    
    def query_persona(self, persona_name: str, question: str) -> Dict:
        """
        íŠ¹ì • í˜ë¥´ì†Œë‚˜ì—ê²Œ ì§ˆë¬¸ (LangChain 1.0 LCEL ë°©ì‹)
        
        Args:
            persona_name: í˜ë¥´ì†Œë‚˜ ì´ë¦„
            question: ì§ˆë¬¸
        
        Returns:
            ë‹µë³€ ë° ì¶œì²˜ ë¬¸ì„œ
        """
        if persona_name not in self.retrievers:
            return {
                'persona': persona_name,
                'answer': f"í˜ë¥´ì†Œë‚˜ '{persona_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                'source_documents': []
            }
        
        # 1. Retrieverë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (LangChain 1.0 - invoke ì‚¬ìš©)
        retriever = self.retrievers[persona_name]
        docs = retriever.invoke(question)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # 3. Prompt ìƒì„± (LangChain 1.0 ë°©ì‹)
        prompt_template = f"""ë‹¹ì‹ ì€ {self.personas[persona_name]}ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì‹¤ì œ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
í†µê³„ì™€ ì‹¤ì œ ë°œì–¸ì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ë˜, í˜ë¥´ì†Œë‚˜ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì„¸ìš”.

[ì‹¤ì œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸]
{{context}}

[ì§ˆë¬¸]
{{question}}

[ë‹µë³€ ì§€ì¹¨]
- ì‹¤ì œ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì œì‹œ
- í†µê³„ ìˆ˜ì¹˜ í™œìš©
- ì‹¤ì œ ì‚¬ìš©ì ë°œì–¸ ì¸ìš©
- í˜ë¥´ì†Œë‚˜ í†¤ ìœ ì§€

ë‹µë³€:"""
        
        prompt = ChatPromptTemplate.from_template(prompt_template)
        
        # 4. LCEL Chain êµ¬ì„±
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)
        
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        # 5. Chain ì‹¤í–‰
        answer = rag_chain.invoke(question)
        
        return {
            'persona': self.personas[persona_name],
            'answer': answer,
            'source_documents': [
                doc.page_content[:200] + "..." 
                for doc in docs
            ],
            'full_source_documents': docs
        }
    
    def get_retriever(self, persona_name: str):
        """
        íŠ¹ì • í˜ë¥´ì†Œë‚˜ì˜ retriever ê°€ì ¸ì˜¤ê¸°
        
        Args:
            persona_name: í˜ë¥´ì†Œë‚˜ ì´ë¦„
        
        Returns:
            LangChain Retriever ê°ì²´
        """
        return self.retrievers.get(persona_name)

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ğŸš€ RAG Manager í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")
    
    # RAG Manager ì´ˆê¸°í™” (OpenAI Embeddings ì‚¬ìš©)
    rag = RAGManager(use_openai_embeddings=True)
    
    # ëª¨ë“  í˜ë¥´ì†Œë‚˜ ë¡œë“œ
    rag.load_all_personas()
    
    print("\n" + "="*80)
    print("ğŸ§ª RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*80 + "\n")
    
    # í…ŒìŠ¤íŠ¸ 1: get_context() ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ 1: get_context() ë©”ì„œë“œ")
    print("-"*80)
    
    test_query = "ìƒíƒœê³„ ì „í™˜ì´ ì–´ë µì§€ ì•Šì•˜ë‚˜ìš”?"
    contexts = rag.get_context('customer_iphone_to_galaxy', test_query, k=2)
    
    print(f"ì§ˆì˜: {test_query}")
    print(f"í˜ë¥´ì†Œë‚˜: {rag.personas['customer_iphone_to_galaxy']}")
    print(f"\nê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ({len(contexts)}ê°œ):\n")
    for i, context in enumerate(contexts, 1):
        print(f"[{i}] {context[:200]}...\n")
    
    # í…ŒìŠ¤íŠ¸ 2: query_persona() ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ 2: query_persona() ë©”ì„œë“œ")
    print("-"*80 + "\n")
    
    test_questions = [
        ("customer_iphone_to_galaxy", "ì•„ì´í°ì—ì„œ ê°¤ëŸ­ì‹œë¡œ ë°”ê¾¸ë©´ ì–´ë–¤ ì ì´ ì¢‹ì•„ìš”?"),
        ("employee_marketer", "iPhone ì‚¬ìš©ìë¥¼ Galaxyë¡œ ì „í™˜ì‹œí‚¤ëŠ” ë§ˆì¼€íŒ… ì „ëµì€?"),
        ("employee_developer", "í´ë”ë¸” ì•± í˜¸í™˜ì„± ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆë‚˜ìš”?"),
    ]
    
    for persona, question in test_questions:
        print(f"\n{'='*80}")
        print(f"í˜ë¥´ì†Œë‚˜: {rag.personas[persona]}")
        print(f"ì§ˆë¬¸: {question}")
        print('='*80)
        
        result = rag.query_persona(persona, question)
        print(f"\nğŸ’¬ ë‹µë³€:\n{result['answer']}")
        print(f"\nğŸ“š ì¶œì²˜ ë¬¸ì„œ ìˆ˜: {len(result['source_documents'])}")
        if result['source_documents']:
            print(f"\nğŸ“„ ì¶œì²˜ ë¯¸ë¦¬ë³´ê¸°:")
            for i, doc in enumerate(result['source_documents'][:2], 1):
                print(f"   [{i}] {doc}")
    
    # í…ŒìŠ¤íŠ¸ 3: Retriever ì§ì ‘ í…ŒìŠ¤íŠ¸
    print("\n\n" + "="*80)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ 3: Retriever ì§ì ‘ ì‚¬ìš©")
    print("-"*80 + "\n")
    
    retriever = rag.get_retriever('employee_designer')
    if retriever:
        query = "ë””ìì¸ ì² í•™ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        docs = retriever.get_relevant_documents(query)
        print(f"ì§ˆì˜: {query}")
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        print(f"\nìƒìœ„ ë¬¸ì„œ:\n{docs[0].page_content[:300]}...\n")
    
    print("="*80)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)
